{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#cleaning"
      ],
      "metadata": {
        "id": "oL7f8V6-Il7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGzP3Qs0If-L",
        "outputId": "e990aecb-1741-45df-bb57-be204324548b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Structural sanity checks ===\n",
            "=== Unit variant summary (before vs after cleaning) ===\n",
            "inch-variant '\\\\bInch\\\\b': before=172, after=0\n",
            "inch-variant '\\\\bINCH\\\\b': before=0, after=0\n",
            "inch-variant '\\\\binches\\\\b': before=605, after=0\n",
            "inch-variant '[\"”″]': before=11077, after=2\n",
            "inch-variant '-inch': before=12, after=0\n",
            "hz-variant   '\\\\bHertz\\\\b': before=0, after=0\n",
            "hz-variant   '\\\\bhertz\\\\b': before=0, after=0\n",
            "hz-variant   '\\\\bHz\\\\b': before=305, after=0\n",
            "hz-variant   '\\\\bHZ\\\\b': before=0, after=0\n",
            "hz-variant   '-hz': before=0, after=0\n",
            "canonical 'inch' after cleaning: 11780\n",
            "canonical 'hz'   after cleaning: 2970\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "# core cleaning for a single string\n",
        "\n",
        "def clean_units_text(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalise inch / hertz representations exactly as in Table 1:\n",
        "      Inch, inches, ″, -inch, inch to inch\n",
        "      Hertz, hertz, Hz, HZ, hz, -hz to hz\n",
        "\n",
        "    Also handles attached forms: 47\", 47-inch, 240Hz, 240 HZ, etc.\n",
        "    Then lowercases the entire string.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "\n",
        "    #  INCH normalisation\n",
        "\n",
        "    # 0) any number followed by a quote / double-prime to \"<num> inch\"\n",
        "    s = re.sub(\n",
        "        r'(\\d+(?:\\.\\d+)?)\\s*[\"”″]',\n",
        "        r'\\1 inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # number followed by Inch / inches (no or some space)\n",
        "    s = re.sub(\n",
        "        r'(\\d+(?:\\.\\d+)?)[ ]*(Inch|INCH|inch|inches|Inches)\\b',\n",
        "        r'\\1 inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # number followed by \"-inch\" variants to \"<num> inch\"\n",
        "    s = re.sub(\n",
        "        r'(\\d+(?:\\.\\d+)?)[ ]*-\\s*(Inch|INCH|inch|inches|Inches)\\b',\n",
        "        r'\\1 inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # bare \"-inch\" token to \"inch\"\n",
        "    s = re.sub(\n",
        "        r'-\\s*(Inch|INCH|inch|inches|Inches)\\b',\n",
        "        'inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # standalone Inch / inches to inch\n",
        "    s = re.sub(\n",
        "        r'\\b(Inch|INCH|inch|inches|Inches)\\b',\n",
        "        'inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # standalone quote / double-prime token to inch\n",
        "    s = re.sub(\n",
        "        r'\\b[\"”″]\\b',\n",
        "        'inch',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    #  HERTZ normalisation\n",
        "\n",
        "    # number followed by Hertz / Hz variants to \"<num> hz\"\n",
        "    s = re.sub(\n",
        "        r'(\\d+(?:\\.\\d+)?)[ ]*(Hertz|hertz|Hz|HZ|hz)\\b',\n",
        "        r'\\1 hz',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # number followed by \"-hz\" to \"<num> hz\"\n",
        "    s = re.sub(\n",
        "        r'(\\d+(?:\\.\\d+)?)[ ]*-\\s*(hz|HZ|Hz|hZ)\\b',\n",
        "        r'\\1 hz',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # 3) bare \"-hz\" token to \"hz\"\n",
        "    s = re.sub(\n",
        "        r'-\\s*(hz|HZ|Hz|hZ)\\b',\n",
        "        'hz',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # 4) standalone Hertz / Hz / hz to hz\n",
        "    s = re.sub(\n",
        "        r'\\b(Hertz|hertz|Hz|HZ|hz)\\b',\n",
        "        'hz',\n",
        "        s\n",
        "    )\n",
        "\n",
        "    # ---- lower-case everything (final step) ----\n",
        "    s = s.lower()\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------- cleaning a single product record ----------\n",
        "\n",
        "def clean_product_record(record: dict) -> dict:\n",
        "    rec = deepcopy(record)\n",
        "\n",
        "    if \"title\" in rec and isinstance(rec[\"title\"], str):\n",
        "        rec[\"title\"] = clean_units_text(rec[\"title\"])\n",
        "\n",
        "    if \"featuresMap\" in rec and isinstance(rec[\"featuresMap\"], dict):\n",
        "        cleaned_features = {}\n",
        "        for k, v in rec[\"featuresMap\"].items():\n",
        "            cleaned_features[k] = clean_units_text(v) if isinstance(v, str) else v\n",
        "        rec[\"featuresMap\"] = cleaned_features\n",
        "\n",
        "    return rec\n",
        "\n",
        "\n",
        "# ---------- cleaning the full TVs-all-merged structure ----------\n",
        "\n",
        "def clean_tv_dataset(tv_data: dict) -> dict:\n",
        "    cleaned = {}\n",
        "    for model_id, records in tv_data.items():\n",
        "        new_records = []\n",
        "        for rec in records:\n",
        "            if isinstance(rec, dict):\n",
        "                new_records.append(clean_product_record(rec))\n",
        "            else:\n",
        "                new_records.append(rec)\n",
        "        cleaned[model_id] = new_records\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# ---------- helpers for checks ----------\n",
        "\n",
        "INCH_VARIANT_PATTERNS = [\n",
        "    r'\\bInch\\b', r'\\bINCH\\b', r'\\binches\\b',\n",
        "    r'[\"”″]', r'-inch',\n",
        "]\n",
        "HERTZ_VARIANT_PATTERNS = [\n",
        "    r'\\bHertz\\b', r'\\bhertz\\b', r'\\bHz\\b', r'\\bHZ\\b', r'-hz',\n",
        "]\n",
        "\n",
        "\n",
        "def iter_all_strings(tv_data: dict):\n",
        "    for model_id, records in tv_data.items():\n",
        "        for idx, rec in enumerate(records):\n",
        "            title = rec.get(\"title\")\n",
        "            if isinstance(title, str):\n",
        "                yield (model_id, idx, \"title\", title)\n",
        "            fmap = rec.get(\"featuresMap\")\n",
        "            if isinstance(fmap, dict):\n",
        "                for k, v in fmap.items():\n",
        "                    if isinstance(v, str):\n",
        "                        yield (model_id, idx, f\"featuresMap[{k}]\", v)\n",
        "\n",
        "\n",
        "def count_pattern_occurrences(tv_data: dict, pattern: str) -> int:\n",
        "    regex = re.compile(pattern)\n",
        "    count = 0\n",
        "    for _, _, _, text in iter_all_strings(tv_data):\n",
        "        count += len(regex.findall(text))\n",
        "    return count\n",
        "\n",
        "\n",
        "def unit_variant_summary(tv_raw: dict, tv_clean: dict):\n",
        "    print(\"=== Unit variant summary (before vs after cleaning) ===\")\n",
        "    for pat in INCH_VARIANT_PATTERNS:\n",
        "        before = count_pattern_occurrences(tv_raw, pat)\n",
        "        after = count_pattern_occurrences(tv_clean, pat)\n",
        "        print(f\"inch-variant {pat!r}: before={before}, after={after}\")\n",
        "\n",
        "    for pat in HERTZ_VARIANT_PATTERNS:\n",
        "        before = count_pattern_occurrences(tv_raw, pat)\n",
        "        after = count_pattern_occurrences(tv_clean, pat)\n",
        "        print(f\"hz-variant   {pat!r}: before={before}, after={after}\")\n",
        "\n",
        "    inch_canonical = count_pattern_occurrences(tv_clean, r'\\binch\\b')\n",
        "    hz_canonical = count_pattern_occurrences(tv_clean, r'\\bhz\\b')\n",
        "    print(f\"canonical 'inch' after cleaning: {inch_canonical}\")\n",
        "    print(f\"canonical 'hz'   after cleaning: {hz_canonical}\")\n",
        "\n",
        "\n",
        "def structural_sanity_checks(tv_raw: dict, tv_clean: dict):\n",
        "    print(\"\\n=== Structural sanity checks ===\")\n",
        "    assert set(tv_raw.keys()) == set(tv_clean.keys()), \"modelID sets differ\"\n",
        "\n",
        "    for model_id in tv_raw:\n",
        "        raw_list = tv_raw[model_id]\n",
        "        clean_list = tv_clean[model_id]\n",
        "        assert len(raw_list) == len(clean_list), f\"length mismatch for {model_id}\"\n",
        "\n",
        "        for raw_rec, clean_rec in zip(raw_list, clean_list):\n",
        "            for field in (\"modelID\", \"shop\", \"url\"):\n",
        "                if field in raw_rec:\n",
        "                    if raw_rec[field] != clean_rec.get(field):\n",
        "                        raise AssertionError(f\"{field} changed for modelID={model_id}\")\n",
        "\n",
        "# ---------- example I/O usage + checks ----------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "\n",
        "    path_in = Path(\"TVs-all-merged.json\")\n",
        "    path_out = Path(\"TVs-all-merged-cleaned.json\")\n",
        "\n",
        "    with path_in.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        tv_raw = json.load(f)\n",
        "\n",
        "    tv_clean = clean_tv_dataset(tv_raw)\n",
        "\n",
        "    with path_out.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(tv_clean, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    structural_sanity_checks(tv_raw, tv_clean)\n",
        "    unit_variant_summary(tv_raw, tv_clean)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mw extraction"
      ],
      "metadata": {
        "id": "HfkX4XMfInwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import numpy as np\n",
        "\n",
        "# REGEX DEFINITIONS\n",
        "TITLE_MW_RE = re.compile(\n",
        "    r\"^[a-zA-Z0-9]*((?:[0-9]+[^0-9, ]+)|(?:[^0-9, ]+[0-9]+))[a-zA-Z0-9]*$\"\n",
        ")\n",
        "\n",
        "VALUE_MW_RE = re.compile(\n",
        "    r\"^(?:\\d+(?:\\.\\d+)?[a-zA-Z]+|\\d+(?:\\.\\d+)?)$\"\n",
        ")\n",
        "\n",
        "VALUE_TOKEN_RE = re.compile(r\"[A-Za-z0-9\\.]+\")\n",
        "\n",
        "\n",
        "def extract_title_model_words(title: str) -> set:\n",
        "    if title is None:\n",
        "        title = \"\"\n",
        "\n",
        "    mw_set: set = set()\n",
        "\n",
        "    # simple tokenization: split on whitespace, strip punctuation\n",
        "    for raw_tok in title.split():\n",
        "        tok = raw_tok.strip(\" ,.;:/()[]{}<>\\\"'“”’+-\")\n",
        "        if not tok:\n",
        "            continue\n",
        "        if TITLE_MW_RE.match(tok):\n",
        "            mw_set.add(tok)\n",
        "\n",
        "    return mw_set\n",
        "\n",
        "def extract_value_model_words_from_string(value: str) -> set:\n",
        "    if value is None:\n",
        "        value = \"\"\n",
        "\n",
        "    mw_set: set = set()\n",
        "    for tok in VALUE_TOKEN_RE.findall(str(value)):\n",
        "        tok = tok.strip()\n",
        "        if not tok:\n",
        "            continue\n",
        "\n",
        "        if VALUE_MW_RE.match(tok):\n",
        "            m_num = re.match(r\"^(\\d+(?:\\.\\d+)?)\", tok)\n",
        "            if m_num:\n",
        "                mw_set.add(m_num.group(1))\n",
        "    return mw_set\n",
        "\n",
        "\n",
        "def extract_value_model_words_from_product(features_map: Dict[str, Any]) -> set:\n",
        "    mw_set: set = set()\n",
        "    for v in features_map.values():\n",
        "        mw_set.update(extract_value_model_words_from_string(str(v)))\n",
        "    return mw_set\n"
      ],
      "metadata": {
        "id": "uj8Fo5DVIpxq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vectorization"
      ],
      "metadata": {
        "id": "16dyMIFkIqIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Set\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, Any, List, Tuple, Set\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PATHS\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "CLEANED_PATH      = Path(\"TVs-all-merged-cleaned.json\")  # output of cleaning step\n",
        "BINARY_OUT_NPZ    = Path(\"TVs-binary-vectors.npz\")\n",
        "BINARY_OUT_JSON   = Path(\"TVs-products-with-binary.json\")  # optional, for inspection\n",
        "\n",
        "def build_global_vocab(MW_title: List[str], MW_value: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    MW = union(MW_title, MW_value)\n",
        "    Returns:\n",
        "      - vocab: list of model words (fixed order)\n",
        "      - index: dict model_word -> column index\n",
        "    \"\"\"\n",
        "    MW: Set[str] = set(MW_title) | set(MW_value)\n",
        "    vocab = sorted(MW)\n",
        "    index = {mw: j for j, mw in enumerate(vocab)}\n",
        "    return vocab, index\n",
        "\n",
        "def build_binary_matrix(\n",
        "    products: List[Dict[str, Any]],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build dense binary matrix X of shape (n_products, n_modelwords)\n",
        "    X[i, j] = 1 if product i contains model word j (in title or value), else 0.\n",
        "    \"\"\"\n",
        "    n_products = len(products)\n",
        "    n_mw = len(vocab_index)\n",
        "\n",
        "    X = np.zeros((n_products, n_mw), dtype=np.uint8)\n",
        "\n",
        "    for i, prod in enumerate(products):\n",
        "        title_mw = set(prod.get(\"title_model_words\", []) or [])\n",
        "        value_mw = set(prod.get(\"value_model_words\", []) or [])\n",
        "        present = title_mw | value_mw\n",
        "\n",
        "        for mw in present:\n",
        "            j = vocab_index.get(mw)\n",
        "            if j is not None:\n",
        "                X[i, j] = 1\n",
        "\n",
        "    return X\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_products_title_only_auto_maxdf(\n",
        "    tv_clean: Dict[str, list],\n",
        "    min_df: int = 1,\n",
        "    max_models: int = 5,\n",
        ") -> Tuple[List[Dict[str, Any]], List[str], int]:\n",
        "    \"\"\"\n",
        "    1) First pass:\n",
        "       extract raw title_model_words per product\n",
        "       compute df_title[token] over products\n",
        "       compute token_models[token] = set of model_ids\n",
        "\n",
        "    2) Choose max_df as the largest DF among tokens that appear\n",
        "       in <= max_models distinct model_ids.\n",
        "\n",
        "    3) Second pass:\n",
        "       filter per product using min_df and this max_df\n",
        "       build products and MW_title\n",
        "\n",
        "    Returns\n",
        "\n",
        "    products : list of product dicts with: id, model_key, shop, url, title_clean, FeaturesMap_clean, title_model_words (filtered)\n",
        "    MW_title : sorted list of kept title tokens (global vocab)\n",
        "    max_df   : auto-chosen max_df value\n",
        "    \"\"\"\n",
        "    raw_products: List[Dict[str, Any]] = []\n",
        "    df_title = Counter()\n",
        "    token_models: Dict[str, Set[str]] = defaultdict(set)\n",
        "\n",
        "    pid = 0\n",
        "\n",
        "    # first pass: raw tokens + DF + models per token\n",
        "    for model_id, records in tv_clean.items():\n",
        "        for rec in records:\n",
        "            if not isinstance(rec, dict):\n",
        "                continue\n",
        "\n",
        "            title_clean = rec.get(\"title\", \"\") or \"\"\n",
        "            fmap_clean  = rec.get(\"featuresMap\", {}) or {}\n",
        "\n",
        "            mw_title = sorted(extract_title_model_words(title_clean))\n",
        "\n",
        "            # DF per product (not per occurrence)\n",
        "            unique_toks = set(mw_title)\n",
        "            for tok in unique_toks:\n",
        "                df_title[tok] += 1\n",
        "                token_models[tok].add(model_id)\n",
        "\n",
        "            raw_products.append(\n",
        "                {\n",
        "                    \"id\": pid,\n",
        "                    \"model_key\": model_id,\n",
        "                    \"shop\": rec.get(\"shop\"),\n",
        "                    \"url\": rec.get(\"url\"),\n",
        "                    \"title_clean\": title_clean,\n",
        "                    \"featuresMap_clean\": fmap_clean,\n",
        "                    \"title_model_words\": mw_title,   # unfiltered for now\n",
        "                }\n",
        "            )\n",
        "            pid += 1\n",
        "\n",
        "    # derive max_df from tokens with small model support\n",
        "    # candidates: tokens that:\n",
        "    # appear in at least min_df products\n",
        "    # appear in <= max_models distinct models\n",
        "    candidate_dfs = []\n",
        "    for tok, df in df_title.items():\n",
        "        if df >= min_df and len(token_models[tok]) <= max_models:\n",
        "            candidate_dfs.append(df)\n",
        "\n",
        "    max_df = max(candidate_dfs)\n",
        "    print(max_df)\n",
        "    #else:\n",
        "        # fallback: if nothing matches, pick a conservative upper bound\n",
        "        # e.g. 1% of products\n",
        "        #n_products = len(raw_products)\n",
        "        #max_df = max(min_df, int(0.01 * n_products))\n",
        "\n",
        "    # second pass: filter per product with min_df, max_df\n",
        "    allowed_title = {\n",
        "        tok for tok, df in df_title.items()\n",
        "        if min_df <= df <= max_df\n",
        "    }\n",
        "\n",
        "    products: List[Dict[str, Any]] = []\n",
        "    MW_title_set: Set[str] = set()\n",
        "\n",
        "    for p in raw_products:\n",
        "        ft = [tok for tok in p[\"title_model_words\"] if tok in allowed_title]\n",
        "\n",
        "        p2 = dict(p)\n",
        "        p2[\"title_model_words\"] = ft\n",
        "        products.append(p2)\n",
        "\n",
        "        MW_title_set.update(ft)\n",
        "\n",
        "    MW_title = sorted(MW_title_set)\n",
        "\n",
        "    return products, MW_title, max_df\n",
        "\n",
        "def main():\n",
        "    # tv_clean must be loaded from CLEANED_PATH before\n",
        "    with CLEANED_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        tv_clean = json.load(f)\n",
        "\n",
        "    products, MW_title, max_df = build_products_title_only_auto_maxdf(\n",
        "    tv_clean,\n",
        "    min_df=2,\n",
        "    max_models=5,\n",
        "    )\n",
        "\n",
        "    # build vocab + X\n",
        "    vocab = MW_title\n",
        "    vocab_index = {mw: j for j, mw in enumerate(vocab)}\n",
        "    X = build_binary_matrix(products, vocab_index)\n",
        "\n",
        "    product_ids  = np.array([p[\"id\"] for p in products], dtype=int)\n",
        "    model_ids    = np.array([p[\"model_key\"] for p in products], dtype=object)\n",
        "    shops        = np.array([p.get(\"shop\") for p in products], dtype=object)\n",
        "    urls         = np.array([p.get(\"url\") for p in products], dtype=object)\n",
        "\n",
        "    np.savez_compressed(\n",
        "        BINARY_OUT_NPZ,\n",
        "        X=X,\n",
        "        vocab=np.array(vocab, dtype=object),\n",
        "        product_ids=product_ids,\n",
        "        model_ids=model_ids,\n",
        "        shops=shops,\n",
        "        urls=urls,\n",
        "    )\n",
        "\n",
        "    print(f\"#products: {X.shape[0]}\")\n",
        "    print(f\"#model words (dim): {X.shape[1]}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIkKh404IwfZ",
        "outputId": "b891f9ab-5511-4ac2-a126-e8bb28412234"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "#products: 1624\n",
            "#model words (dim): 269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#minhashing"
      ],
      "metadata": {
        "id": "Wp2kL0YWIw-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MinHash stage.\n",
        "\n",
        "Input  : TVs-binary-vectors.npz  (output of the model-word / binary-vector step)\n",
        "Output : TVs-minhash-signatures.npz containing\n",
        "         - signatures : (num_hashes, n_products) int64\n",
        "         - a, b, prime: hash function parameters\n",
        "         - product_ids, model_ids, shops, urls, vocab  (copied through)\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# PATHS + BASIC PARAMETERS\n",
        "BINARY_NPZ_PATH   = Path(\"TVs-binary-vectors.npz\")\n",
        "MINHASH_NPZ_PATH  = Path(\"TVs-minhash-signatures.npz\")\n",
        "\n",
        "NUM_HASHES = 360\n",
        "RNG_SEED   = 12345\n",
        "\n",
        "\n",
        "def _next_prime(n: int) -> int:\n",
        "    \"\"\"Small helper: return the smallest prime >= n+1.\"\"\"\n",
        "    def is_prime(k: int) -> bool:\n",
        "        if k <= 3:\n",
        "            return k >= 2\n",
        "        if k % 2 == 0:\n",
        "            return False\n",
        "        i = 3\n",
        "        while i * i <= k:\n",
        "            if k % i == 0:\n",
        "                return False\n",
        "            i += 2\n",
        "        return True\n",
        "\n",
        "    p = max(2, n + 1)\n",
        "    while not is_prime(p):\n",
        "        p += 1\n",
        "    return p\n",
        "\n",
        "def compute_minhash_signatures(\n",
        "    X_binary: np.ndarray,\n",
        "    num_hashes: int,\n",
        "    rng_seed: int = RNG_SEED,\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    X_binary : (n_products, n_modelwords) in {0,1}\n",
        "               rows   to columns in the MinHash slide\n",
        "               cols   to rows (r) in the MinHash slide\n",
        "    num_hashes : number of hash functions t\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    signatures : (num_hashes, n_products) array\n",
        "                 column j is the MinHash signature for product j\n",
        "    a, b       : (num_hashes,) arrays with hash parameters\n",
        "    prime      : int, modulus used in hash functions\n",
        "    \"\"\"\n",
        "    # Characteristic matrix with rows = model words, cols = products\n",
        "    C = X_binary.T.astype(np.uint8)          # shape: (n_rows, n_cols)\n",
        "    n_rows, n_cols = C.shape\n",
        "\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "    prime = _next_prime(n_rows)\n",
        "    a = rng.integers(1, prime, size=num_hashes, dtype=np.int64)\n",
        "    b = rng.integers(0, prime, size=num_hashes, dtype=np.int64)\n",
        "\n",
        "    # Initialise signature matrix with +inf (max int)\n",
        "    max_int = np.iinfo(np.int64).max\n",
        "    signatures = np.full((num_hashes, n_cols), max_int, dtype=np.int64)\n",
        "\n",
        "    # for each row r\n",
        "    for r in range(n_rows):\n",
        "        row = C[r, :]\n",
        "        cols_with_1 = np.nonzero(row)[0]\n",
        "        if cols_with_1.size == 0:\n",
        "            continue\n",
        "\n",
        "        # for each hash function hi compute hi(r)\n",
        "        # hi(r) = (a_i * r + b_i) mod prime\n",
        "        r_val = int(r)\n",
        "        hash_vals = (a * r_val + b) % prime          # shape: (num_hashes,)\n",
        "\n",
        "        # for each column c that has 1 in row r:\n",
        "        #   if hi(r) < M(i,c) then M(i,c) := hi(r)\n",
        "        signatures[:, cols_with_1] = np.minimum(\n",
        "            signatures[:, cols_with_1],\n",
        "            hash_vals[:, None],\n",
        "        )\n",
        "\n",
        "    return signatures, a, b, prime\n",
        "\n",
        "def run_minhash_stage(\n",
        "    binary_npz_path: Path = BINARY_NPZ_PATH,\n",
        "    out_npz_path: Path = MINHASH_NPZ_PATH,\n",
        "    num_hashes: int = NUM_HASHES,\n",
        "    rng_seed: int = RNG_SEED,\n",
        ") -> None:\n",
        "    # load binary vectors with metadata (modelID etc.)\n",
        "    data = np.load(binary_npz_path, allow_pickle=True)\n",
        "\n",
        "    X          = data[\"X\"]              # (n_products, n_modelwords)\n",
        "    vocab      = data[\"vocab\"]\n",
        "    product_ids = data[\"product_ids\"]\n",
        "    model_ids   = data[\"model_ids\"]\n",
        "    shops       = data[\"shops\"]\n",
        "    urls        = data[\"urls\"]\n",
        "\n",
        "    signatures, a, b, prime = compute_minhash_signatures(\n",
        "        X_binary=X,\n",
        "        num_hashes=num_hashes,\n",
        "        rng_seed=rng_seed,\n",
        "    )\n",
        "\n",
        "    # persist signatures + hash params + metadata for later LSH/MSM/eval\n",
        "    np.savez_compressed(\n",
        "        out_npz_path,\n",
        "        signatures=signatures,\n",
        "        a=a,\n",
        "        b=b,\n",
        "        prime=np.array(prime, dtype=np.int64),\n",
        "        vocab=vocab,\n",
        "        product_ids=product_ids,\n",
        "        model_ids=model_ids,   #carried through for later evaluation\n",
        "        shops=shops,\n",
        "        urls=urls,\n",
        "    )\n",
        "\n",
        "    print(f\"[MinHash] #products: {signatures.shape[1]}\")\n",
        "    print(f\"[MinHash] #hash functions (signature length): {signatures.shape[0]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dz_mXrNEI4Ob"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lsh"
      ],
      "metadata": {
        "id": "3JbQ-0-EI4dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "MINHASH_NPZ_PATH = Path(\"TVs-minhash-signatures.npz\")\n",
        "LSH_OUT_NPZ_PATH = Path(\"TVs-lsh-candidates.npz\")\n",
        "\n",
        "def all_factor_pairs(n: int) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    All (r, b) with r * b = n.\n",
        "    r = rows per band, b = number of bands.\n",
        "    \"\"\"\n",
        "    out: List[Tuple[int, int]] = []\n",
        "    for r in range(1, n + 1):\n",
        "        if n % r == 0:\n",
        "            b = n // r\n",
        "            out.append((r, b))\n",
        "    return out\n",
        "\n",
        "\n",
        "def choose_b_r_for_t(\n",
        "    n_hashes: int,\n",
        "    target_t: float,\n",
        "    max_slack: int = 3,\n",
        ") -> Tuple[int, int, float, int]:\n",
        "    \"\"\"\n",
        "    Given signature length n_hashes and desired threshold t,\n",
        "    pick (r, b) with r * b = m where m is in [n_hashes - max_slack, n_hashes]\n",
        "    such that t_hat = (1 / b) ** (1 / r) is closest to target_t.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    r : rows per band\n",
        "    b : number of bands\n",
        "    t_hat : resulting effective threshold (1 / b) ** (1 / r)\n",
        "    used_n_hashes : m actually used (<= n_hashes)\n",
        "    \"\"\"\n",
        "    best_r, best_b, best_t_hat = None, None, None\n",
        "    best_diff = float(\"inf\")\n",
        "    best_m = None\n",
        "\n",
        "    lo = max(1, n_hashes - max_slack)\n",
        "    hi = n_hashes                      # <-- no + max_slack\n",
        "\n",
        "    for m in range(lo, hi + 1):\n",
        "        for r, b in all_factor_pairs(m):\n",
        "            t_hat = (1.0 / b) ** (1.0 / r)\n",
        "            diff = abs(t_hat - target_t)\n",
        "            if diff < best_diff:\n",
        "                best_diff = diff\n",
        "                best_r, best_b, best_t_hat, best_m = r, b, t_hat, m\n",
        "\n",
        "    return best_r, best_b, best_t_hat, best_m\n",
        "\n",
        "def lsh_candidate_pairs(signatures: np.ndarray, bands: int, rows_per_band: int) -> np.ndarray:\n",
        "    m, n = signatures.shape\n",
        "    assert bands * rows_per_band <= m\n",
        "\n",
        "    buckets = {}\n",
        "\n",
        "    for band in range(bands):\n",
        "        start = band * rows_per_band\n",
        "        end   = start + rows_per_band\n",
        "\n",
        "        band_block = signatures[start:end, :]  # (rows_per_band, n)\n",
        "\n",
        "        for j in range(n):\n",
        "            # IMPORTANT: include band in key\n",
        "            key = (band, tuple(band_block[:, j].tolist()))\n",
        "            buckets.setdefault(key, []).append(j)\n",
        "\n",
        "    # collect candidate pairs (unique)\n",
        "    pairs = set()\n",
        "    for idxs in buckets.values():\n",
        "        if len(idxs) < 2:\n",
        "            continue\n",
        "        idxs = sorted(idxs)\n",
        "        for a in range(len(idxs)):\n",
        "            ia = idxs[a]\n",
        "            for b in range(a + 1, len(idxs)):\n",
        "                ib = idxs[b]\n",
        "                pairs.add((ia, ib))\n",
        "\n",
        "    if not pairs:\n",
        "        return np.empty((0, 2), dtype=int)\n",
        "    return np.array(sorted(pairs), dtype=int)\n",
        "\n",
        "def minhash_jaccard(\n",
        "    signatures: np.ndarray,\n",
        "    pairs: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    signatures : (num_hashes, n_products)\n",
        "    pairs      : (K, 2) array of indices (i, j)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sim : (K,) array with estimated Jaccard similarities\n",
        "          = fraction of rows where signatures[:, i] == signatures[:, j]\n",
        "    \"\"\"\n",
        "    num_hashes = signatures.shape[0]\n",
        "    sims = np.empty(pairs.shape[0], dtype=float)\n",
        "\n",
        "    for idx, (i, j) in enumerate(pairs):\n",
        "        sims[idx] = np.mean(signatures[:, i] == signatures[:, j])\n",
        "\n",
        "    return sims\n",
        "\n",
        "def run_lsh_stage(\n",
        "    minhash_npz_path: Path = MINHASH_NPZ_PATH,\n",
        "    out_npz_path: Path = LSH_OUT_NPZ_PATH,\n",
        "    sim_threshold: float = 0.5,\n",
        "    max_slack: int = 3,\n",
        "    bands: int | None = None,\n",
        "    rows_per_band: int | None = None,\n",
        ") -> None:\n",
        "    data = np.load(minhash_npz_path, allow_pickle=True)\n",
        "\n",
        "    signatures   = data[\"signatures\"]          # (num_hashes, n_products)\n",
        "    a            = data[\"a\"]\n",
        "    b_hash       = data[\"b\"]\n",
        "    prime        = int(data[\"prime\"])\n",
        "    vocab        = data[\"vocab\"]\n",
        "    product_ids  = data[\"product_ids\"]\n",
        "    model_ids    = data[\"model_ids\"]\n",
        "    shops        = data[\"shops\"]\n",
        "    urls         = data[\"urls\"]\n",
        "\n",
        "    num_hashes, n_products = signatures.shape\n",
        "\n",
        "    # Auto-tune (r, b) if not given\n",
        "    if bands is None or rows_per_band is None:\n",
        "        r, b, t_hat, used_m = choose_b_r_for_t(\n",
        "            n_hashes=num_hashes,\n",
        "            target_t=sim_threshold,\n",
        "            max_slack=max_slack,\n",
        "        )\n",
        "\n",
        "        if used_m < num_hashes:\n",
        "            signatures = signatures[:used_m, :]\n",
        "            num_hashes = used_m\n",
        "\n",
        "        rows_per_band = r\n",
        "        bands = b\n",
        "\n",
        "        print(\n",
        "            f\"[LSH] Auto params for target t={sim_threshold}: \"\n",
        "            f\"r={rows_per_band}, b={bands}, t_hat={t_hat:.3f}, m={num_hashes}\"\n",
        "        )\n",
        "        '''\n",
        "    else:\n",
        "        assert bands * rows_per_band == num_hashes, (\n",
        "            f\"bands * rows_per_band = {bands * rows_per_band}, \"\n",
        "            f\"but num_hashes = {num_hashes}\"\n",
        "        )\n",
        "      '''\n",
        "    t_hat = (1.0 / bands) ** (1.0 / rows_per_band)\n",
        "\n",
        "    #LSH banding to raw candidate pairs\n",
        "    pairs = lsh_candidate_pairs(\n",
        "        signatures,\n",
        "        bands=bands,\n",
        "        rows_per_band=rows_per_band,\n",
        "    )\n",
        "\n",
        "    # Estimated Jaccard for each candidate\n",
        "    sims = minhash_jaccard(signatures, pairs)\n",
        "\n",
        "    # Apply similarity threshold t\n",
        "    keep_mask = sims >= sim_threshold\n",
        "    pairs_kept = pairs[keep_mask]\n",
        "    sims_kept = sims[keep_mask]\n",
        "\n",
        "    # Map to modelIDs for evaluation\n",
        "    i_idx = pairs_kept[:, 0]\n",
        "    j_idx = pairs_kept[:, 1]\n",
        "\n",
        "    model_ids_i = model_ids[i_idx]\n",
        "    model_ids_j = model_ids[j_idx]\n",
        "    same_model  = (model_ids_i == model_ids_j)\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_npz_path,\n",
        "        i_idx=i_idx,\n",
        "        j_idx=j_idx,\n",
        "        sim_est=sims_kept,\n",
        "        same_model=same_model,\n",
        "        product_ids=product_ids,\n",
        "        model_ids=model_ids,\n",
        "        shops=shops,\n",
        "        urls=urls,\n",
        "        vocab=vocab,\n",
        "        a=a,\n",
        "        b=b_hash,\n",
        "        prime=np.array(prime, dtype=np.int64),\n",
        "        bands=np.array(bands, dtype=int),\n",
        "        rows_per_band=np.array(rows_per_band, dtype=int),\n",
        "        sim_threshold=np.array(sim_threshold, dtype=float),\n",
        "        sim_threshold_effective=np.array(t_hat, dtype=float),\n",
        "    )\n",
        "\n",
        "    print(f\"[LSH] #products: {n_products}\")\n",
        "    print(f\"[LSH] #candidate pairs before threshold: {pairs.shape[0]}\")\n",
        "    print(f\"[LSH] #candidate pairs after threshold t={sim_threshold}: {pairs_kept.shape[0]}\")\n"
      ],
      "metadata": {
        "id": "t_U5DoIdI5fU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#bootstrap and eval"
      ],
      "metadata": {
        "id": "uGKa8Z3OI9qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# PATHS\n",
        "BINARY_NPZ_PATH = Path(\"TVs-binary-vectors.npz\")\n",
        "LSH_NPZ_PATH    = Path(\"TVs-lsh-candidates.npz\")\n",
        "BOOT_OUT_NPZ    = Path(\"TVs-bootstrap-eval.npz\")\n",
        "\n",
        "def exact_jaccard_on_candidates(\n",
        "    X: np.ndarray,\n",
        "    i_idx: np.ndarray,\n",
        "    j_idx: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    X     : (n_products, n_features) binary matrix\n",
        "    i_idx : (K,) candidate indices\n",
        "    j_idx : (K,) candidate indices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sims  : (K,) exact Jaccard similarities\n",
        "    \"\"\"\n",
        "    Xi = X[i_idx]  # (K, p)\n",
        "    Xj = X[j_idx]  # (K, p)\n",
        "\n",
        "    intersection = np.logical_and(Xi, Xj).sum(axis=1)\n",
        "    union        = np.logical_or(Xi, Xj).sum(axis=1)\n",
        "\n",
        "    sims = np.zeros_like(intersection, dtype=float)\n",
        "    nonzero = union > 0\n",
        "    sims[nonzero] = intersection[nonzero] / union[nonzero]\n",
        "    # if union == 0, similarity stays 0\n",
        "    return sims\n",
        "\n",
        "def cluster_with_threshold(\n",
        "    n_items: int,\n",
        "    i_idx: np.ndarray,\n",
        "    j_idx: np.ndarray,\n",
        "    sims: np.ndarray,\n",
        "    theta: float,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    n_items : total number of products\n",
        "    i_idx, j_idx, sims : candidate edges with exact similarity\n",
        "    theta   : distance_threshold in\n",
        "              AgglomerativeClustering with complete linkage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels  : (n_items,) cluster labels\n",
        "              (complete-linkage agglomerative clustering at similarity >= theta)\n",
        "    \"\"\"\n",
        "    # Convert similarities to distances: d = 1 - s\n",
        "    d_edges = 1.0 - np.asarray(sims, dtype=float)\n",
        "\n",
        "    # Build full distance matrix\n",
        "    dist_matrix = np.ones((n_items, n_items), dtype=float)\n",
        "    np.fill_diagonal(dist_matrix, 0.0)\n",
        "\n",
        "    # Fill distances for candidate pairs (symmetric)\n",
        "    i_idx = np.asarray(i_idx, dtype=int)\n",
        "    j_idx = np.asarray(j_idx, dtype=int)\n",
        "    dist_matrix[i_idx, j_idx] = d_edges\n",
        "    dist_matrix[j_idx, i_idx] = d_edges\n",
        "\n",
        "    # Map similarity threshold theta to distance threshold\n",
        "    distance_threshold = 1.0 - float(theta)\n",
        "\n",
        "    # Agglomerative clustering with complete linkage on the precomputed distances\n",
        "    agg = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        metric=\"precomputed\",\n",
        "        linkage=\"complete\",\n",
        "        distance_threshold=distance_threshold,\n",
        "        compute_full_tree=True,\n",
        "    )\n",
        "\n",
        "    labels = agg.fit_predict(dist_matrix)\n",
        "    return labels\n",
        "\n",
        "def pairwise_confusion_and_f1_subset(\n",
        "    labels: np.ndarray,\n",
        "    model_ids: np.ndarray,\n",
        "    subset_idx: np.ndarray,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    labels     : (n,) cluster labels\n",
        "    model_ids  : (n,) gold model IDs\n",
        "    subset_idx : indices to include in evaluation (e.g. OOB set)\n",
        "\n",
        "    TP = predicted duplicate AND same modelID\n",
        "    FP = predicted duplicate AND different modelID\n",
        "    TN = predicted non-duplicate AND different modelID\n",
        "    FN = predicted non-duplicate AND same modelID\n",
        "\n",
        "    'predicted duplicate' = same cluster label.\n",
        "    \"\"\"\n",
        "    idx = np.asarray(subset_idx, dtype=int)\n",
        "    m = idx.size\n",
        "\n",
        "    TP = FP = TN = FN = 0\n",
        "\n",
        "    for a in range(m):\n",
        "        i = idx[a]\n",
        "        li = labels[i]\n",
        "        mi = model_ids[i]\n",
        "        for b in range(a + 1, m):\n",
        "            j = idx[b]\n",
        "            same_cluster = (li == labels[j])\n",
        "            pred_dup = same_cluster\n",
        "            true_dup = (mi == model_ids[j])\n",
        "\n",
        "            if pred_dup and true_dup:\n",
        "                TP += 1\n",
        "            elif pred_dup and not true_dup:\n",
        "                FP += 1\n",
        "            elif (not pred_dup) and true_dup:\n",
        "                FN += 1\n",
        "            else:\n",
        "                TN += 1\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    if precision + recall > 0.0:\n",
        "        F1 = 2.0 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        F1 = 0.0\n",
        "\n",
        "    return {\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"F1\": F1,\n",
        "    }\n",
        "\n",
        "def bootstrap_evaluation(\n",
        "    labels: np.ndarray,\n",
        "    model_ids: np.ndarray,\n",
        "    n_bootstrap: int = 5,\n",
        "    rng_seed: int = 123,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    labels, model_ids defined on ALL n items.\n",
        "\n",
        "    For b = 1..n_bootstrap:\n",
        "        - draw n indices with replacement => in-bag\n",
        "        - OOB = {0..n-1} \\\\ in-bag\n",
        "        - evaluate F1 only on OOB items (out-of-sample)\n",
        "    \"\"\"\n",
        "    n = len(labels)\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "    F1_list = []\n",
        "    prec_list = []\n",
        "    rec_list = []\n",
        "    TP_list, FP_list, TN_list, FN_list = [], [], [], []\n",
        "\n",
        "    for b in range(n_bootstrap):\n",
        "        in_bag = rng.integers(0, n, size=n)\n",
        "        in_bag_mask = np.zeros(n, dtype=bool)\n",
        "        in_bag_mask[in_bag] = True\n",
        "\n",
        "        oob_idx = np.where(~in_bag_mask)[0]\n",
        "\n",
        "        if oob_idx.size < 2:\n",
        "            continue\n",
        "\n",
        "        stats = pairwise_confusion_and_f1_subset(\n",
        "            labels=labels,\n",
        "            model_ids=model_ids,\n",
        "            subset_idx=oob_idx,\n",
        "        )\n",
        "\n",
        "        F1_list.append(stats[\"F1\"])\n",
        "        prec_list.append(stats[\"precision\"])\n",
        "        rec_list.append(stats[\"recall\"])\n",
        "        TP_list.append(stats[\"TP\"])\n",
        "        FP_list.append(stats[\"FP\"])\n",
        "        TN_list.append(stats[\"TN\"])\n",
        "        FN_list.append(stats[\"FN\"])\n",
        "\n",
        "    F1_arr   = np.array(F1_list, dtype=float)\n",
        "    prec_arr = np.array(prec_list, dtype=float)\n",
        "    rec_arr  = np.array(rec_list, dtype=float)\n",
        "\n",
        "    avg_F1  = float(F1_arr.mean()) if F1_arr.size > 0 else 0.0\n",
        "    std_F1  = float(F1_arr.std(ddof=1)) if F1_arr.size > 1 else 0.0\n",
        "    avg_pr  = float(prec_arr.mean()) if prec_arr.size > 0 else 0.0\n",
        "    avg_rec = float(rec_arr.mean()) if rec_arr.size > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"F1_per_boot\": F1_arr,\n",
        "        \"precision_per_boot\": prec_arr,\n",
        "        \"recall_per_boot\": rec_arr,\n",
        "        \"TP_per_boot\": np.array(TP_list, dtype=int),\n",
        "        \"FP_per_boot\": np.array(FP_list, dtype=int),\n",
        "        \"TN_per_boot\": np.array(TN_list, dtype=int),\n",
        "        \"FN_per_boot\": np.array(FN_list, dtype=int),\n",
        "        \"avg_F1\": avg_F1,\n",
        "        \"std_F1\": std_F1,\n",
        "        \"avg_precision\": avg_pr,\n",
        "        \"avg_recall\": avg_rec,\n",
        "        \"n_effective_bootstrap\": F1_arr.size,\n",
        "    }\n",
        "\n",
        "def run_bootstrap_stage(\n",
        "    binary_npz_path: Path = BINARY_NPZ_PATH,\n",
        "    lsh_npz_path: Path = LSH_NPZ_PATH,\n",
        "    out_npz_path: Path = BOOT_OUT_NPZ,\n",
        "    theta: float = 0.9,\n",
        "    n_bootstrap: int = 10,\n",
        "    rng_seed: int = 123,\n",
        ") -> None:\n",
        "    bin_data = np.load(binary_npz_path, allow_pickle=True)\n",
        "    X           = bin_data[\"X\"].astype(bool)\n",
        "    vocab       = bin_data[\"vocab\"]\n",
        "    product_ids = bin_data[\"product_ids\"]\n",
        "    model_ids   = bin_data[\"model_ids\"]\n",
        "    shops       = bin_data[\"shops\"]\n",
        "    urls        = bin_data[\"urls\"]\n",
        "\n",
        "    n_items = X.shape[0]\n",
        "\n",
        "    lsh_data = np.load(lsh_npz_path, allow_pickle=True)\n",
        "    i_idx = lsh_data[\"i_idx\"]\n",
        "    j_idx = lsh_data[\"j_idx\"]\n",
        "\n",
        "    sims_exact = exact_jaccard_on_candidates(X, i_idx, j_idx)\n",
        "\n",
        "    labels = cluster_with_threshold(\n",
        "        n_items=n_items,\n",
        "        i_idx=i_idx,\n",
        "        j_idx=j_idx,\n",
        "        sims=sims_exact,\n",
        "        theta=theta,\n",
        "    )\n",
        "\n",
        "    boot_res = bootstrap_evaluation(\n",
        "        labels=labels,\n",
        "        model_ids=model_ids,\n",
        "        n_bootstrap=n_bootstrap,\n",
        "        rng_seed=rng_seed,\n",
        "    )\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_npz_path,\n",
        "        theta=np.array(theta, dtype=float),\n",
        "        n_bootstrap=np.array(n_bootstrap, dtype=int),\n",
        "        labels=labels,\n",
        "        product_ids=product_ids,\n",
        "        model_ids=model_ids,\n",
        "        shops=shops,\n",
        "        urls=urls,\n",
        "        vocab=vocab,\n",
        "        F1_per_boot=boot_res[\"F1_per_boot\"],\n",
        "        precision_per_boot=boot_res[\"precision_per_boot\"],\n",
        "        recall_per_boot=boot_res[\"recall_per_boot\"],\n",
        "        TP_per_boot=boot_res[\"TP_per_boot\"],\n",
        "        FP_per_boot=boot_res[\"FP_per_boot\"],\n",
        "        TN_per_boot=boot_res[\"TN_per_boot\"],\n",
        "        FN_per_boot=boot_res[\"FN_per_boot\"],\n",
        "        avg_F1=np.array(boot_res[\"avg_F1\"], dtype=float),\n",
        "        std_F1=np.array(boot_res[\"std_F1\"], dtype=float),\n",
        "        avg_precision=np.array(boot_res[\"avg_precision\"], dtype=float),\n",
        "        avg_recall=np.array(boot_res[\"avg_recall\"], dtype=float),\n",
        "        n_effective_bootstrap=np.array(\n",
        "            boot_res[\"n_effective_bootstrap\"], dtype=int\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(f\"[Bootstrap] #products: {n_items}\")\n",
        "    print(f\"[Bootstrap] theta (exact Jaccard): {theta}\")\n",
        "    print(f\"[Bootstrap] requested bootstraps: {n_bootstrap}\")\n",
        "    print(f\"[Bootstrap] effective bootstraps (non-empty OOB): {boot_res['n_effective_bootstrap']}\")\n",
        "    print(f\"[Bootstrap] avg F1 (OOB): {boot_res['avg_F1']:.4f} ± {boot_res['std_F1']:.4f}\")\n",
        "    print(f\"[Bootstrap] avg precision (OOB): {boot_res['avg_precision']:.4f}\")\n",
        "    print(f\"[Bootstrap] avg recall (OOB): {boot_res['avg_recall']:.4f}\")\n"
      ],
      "metadata": {
        "id": "GX-00eLSJAXk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main block"
      ],
      "metadata": {
        "id": "AEkDNe4pZZ2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_pipeline(\n",
        "    # MinHash parameters\n",
        "    minhash_rng_seed: int = 12345,\n",
        "\n",
        "    # LSH parameters\n",
        "    lsh_sim_threshold: float = 0.3,\n",
        "    lsh_max_slack: int = 3,\n",
        "    lsh_bands: int | None = None,\n",
        "    lsh_rows_per_band: int | None = None,\n",
        "\n",
        "    # Exact-Jaccard clustering + bootstrap parameters\n",
        "    theta_exact: float = 0.3,\n",
        "    n_bootstrap: int = 10,\n",
        "    bootstrap_rng_seed: int = 123,\n",
        "\n",
        "    # Path overrides if needed\n",
        "    binary_npz_path: Path = BINARY_NPZ_PATH,\n",
        "    minhash_npz_path: Path = MINHASH_NPZ_PATH,\n",
        "    lsh_npz_path: Path = LSH_NPZ_PATH,\n",
        "    boot_out_npz_path: Path = BOOT_OUT_NPZ,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Unified driver:\n",
        "\n",
        "    0) Determine num_hashes as 50% of binary vector dimension.\n",
        "    1) MinHash signatures from binary vectors.\n",
        "    2) LSH candidate pairs.\n",
        "    3) Exact-Jaccard clustering + bootstrap evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    bin_data = np.load(binary_npz_path, allow_pickle=True)\n",
        "    X = bin_data[\"X\"]\n",
        "    dim = X.shape[1]\n",
        "    num_hashes = 360\n",
        "\n",
        "    print(f\"Binary dimension: {dim}\")\n",
        "    print(f\"Using num_hashes = {num_hashes}\")\n",
        "\n",
        "    print(\"=== Stage 1: MinHash ===\")\n",
        "    run_minhash_stage(\n",
        "        binary_npz_path=binary_npz_path,\n",
        "        out_npz_path=minhash_npz_path,\n",
        "        num_hashes=num_hashes,\n",
        "        rng_seed=minhash_rng_seed,\n",
        "    )\n",
        "\n",
        "    # --- Stage 2: LSH ---\n",
        "    print(\"\\n=== Stage 2: LSH ===\")\n",
        "    run_lsh_stage(\n",
        "        minhash_npz_path=minhash_npz_path,\n",
        "        out_npz_path=lsh_npz_path,\n",
        "        sim_threshold=lsh_sim_threshold,\n",
        "        max_slack=lsh_max_slack,\n",
        "        bands=lsh_bands,\n",
        "        rows_per_band=lsh_rows_per_band,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Stage 3: Exact Jaccard + Bootstrap eval ===\")\n",
        "    run_bootstrap_stage(\n",
        "        binary_npz_path=binary_npz_path,\n",
        "        lsh_npz_path=lsh_npz_path,\n",
        "        out_npz_path=boot_out_npz_path,\n",
        "        theta=theta_exact,\n",
        "        n_bootstrap=n_bootstrap,\n",
        "        rng_seed=bootstrap_rng_seed,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # single place to tweak all experiment parameters\n",
        "    run_full_pipeline(\n",
        "        minhash_rng_seed=12345,\n",
        "\n",
        "        lsh_sim_threshold=0.3,    # LSH similarity threshold on MinHash\n",
        "        lsh_max_slack=0,\n",
        "        lsh_bands=None,           # or set explicit integers\n",
        "        lsh_rows_per_band=None,\n",
        "\n",
        "        theta_exact=0.3,          # exact Jaccard threshold for clustering\n",
        "        n_bootstrap=5,\n",
        "        bootstrap_rng_seed=123,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPD3YhElmVP-",
        "outputId": "8e325aaa-df88-4e7c-c82e-ebddfffb3c84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary dimension: 269\n",
            "Using num_hashes = 360\n",
            "=== Stage 1: MinHash ===\n",
            "[MinHash] #products: 1624\n",
            "[MinHash] #hash functions (signature length): 360\n",
            "\n",
            "=== Stage 2: LSH ===\n",
            "[LSH] Auto params for target t=0.3: r=4, b=90, t_hat=0.325, m=360\n",
            "[LSH] #products: 1624\n",
            "[LSH] #candidate pairs before threshold: 561661\n",
            "[LSH] #candidate pairs after threshold t=0.3: 561659\n",
            "\n",
            "=== Stage 3: Exact Jaccard + Bootstrap eval ===\n",
            "[Bootstrap] #products: 1624\n",
            "[Bootstrap] theta (exact Jaccard): 0.3\n",
            "[Bootstrap] requested bootstraps: 5\n",
            "[Bootstrap] effective bootstraps (non-empty OOB): 5\n",
            "[Bootstrap] avg F1 (OOB): 0.6297 ± 0.0318\n",
            "[Bootstrap] avg precision (OOB): 0.6883\n",
            "[Bootstrap] avg recall (OOB): 0.5820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plot"
      ],
      "metadata": {
        "id": "X7EQPRlNtSJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _fraction_comparisons(n_sub: int, n_cand: int) -> float:\n",
        "    denom = n_sub * (n_sub - 1) // 2\n",
        "    return (n_cand / denom) if denom > 0 else 0.0\n",
        "\n",
        "def all_band_configs(m: int):\n",
        "    cfgs = []\n",
        "    for r in range(1, m + 1):\n",
        "        if m % r == 0:\n",
        "            b = m // r\n",
        "            # implied threshold curve midpoint (paper uses this notion)\n",
        "            # you only need it for labeling/sorting\n",
        "            t_imp = (1.0 / b) ** (1.0 / r)\n",
        "            cfgs.append((t_imp, b, r))\n",
        "    cfgs.sort(key=lambda x: x[0])  # increasing implied threshold\n",
        "    return cfgs\n",
        "\n",
        "def run_one_bootstrap_for_t(\n",
        "    X_bool: np.ndarray,            # (n, p) boolean binary vectors\n",
        "    signatures: np.ndarray,        # (m, n) MinHash signatures for ALL products\n",
        "    model_ids_all: np.ndarray,     # (n,) gold labels\n",
        "    t: float,\n",
        "    theta_exact: float,\n",
        "    rng: np.random.Generator,\n",
        "    max_slack: int = 3,\n",
        "):\n",
        "\n",
        "    #One bootstrap replicate for one LSH threshold t\n",
        "\n",
        "    n = X_bool.shape[0]\n",
        "    in_bag = rng.integers(0, n, size=n)\n",
        "    subset_idx = np.unique(in_bag)\n",
        "    n_sub = subset_idx.size\n",
        "    if n_sub < 2:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # subset views\n",
        "    sig_sub = signatures[:, subset_idx]\n",
        "    X_sub   = X_bool[subset_idx]\n",
        "    mid_sub = model_ids_all[subset_idx]\n",
        "\n",
        "    # choose (r,b) for t\n",
        "    m_hashes = sig_sub.shape[0]\n",
        "    r, b, t_hat, used_m = choose_b_r_for_t(\n",
        "        n_hashes=m_hashes,\n",
        "        target_t=float(t),\n",
        "        max_slack=0,\n",
        "    )\n",
        "    if used_m < m_hashes:\n",
        "        sig_sub = sig_sub[:used_m, :]\n",
        "\n",
        "    # LSH bucket candidates then filtered by estimated sim >= t\n",
        "    raw_pairs = lsh_candidate_pairs(sig_sub, bands=b, rows_per_band=r)\n",
        "    if raw_pairs.shape[0] == 0:\n",
        "        # no comparisons made so F1 zero\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    sim_est = minhash_jaccard(sig_sub, raw_pairs)\n",
        "    keep = sim_est >= float(t)\n",
        "    pairs = raw_pairs[keep]\n",
        "    if pairs.shape[0] == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    frac = _fraction_comparisons(n_sub=n_sub, n_cand=pairs.shape[0])\n",
        "\n",
        "    # exact Jaccard on candidate pairs\n",
        "    i_loc = pairs[:, 0]\n",
        "    j_loc = pairs[:, 1]\n",
        "    sims_exact = exact_jaccard_on_candidates(X_sub, i_loc, j_loc)\n",
        "\n",
        "    # clustering on subset\n",
        "    labels_sub = cluster_with_threshold(\n",
        "        n_items=n_sub,\n",
        "        i_idx=i_loc,\n",
        "        j_idx=j_loc,\n",
        "        sims=sims_exact,\n",
        "        theta=theta_exact,\n",
        "    )\n",
        "\n",
        "    # evaluate final clustering F1 on subset (pairwise)\n",
        "    stats = pairwise_confusion_and_f1_subset(\n",
        "        labels=labels_sub,\n",
        "        model_ids=mid_sub,\n",
        "        subset_idx=np.arange(n_sub),\n",
        "    )\n",
        "    return float(stats[\"F1\"]), float(frac)\n",
        "\n",
        "def paper_style_curve_finalF1_vs_fraction(\n",
        "    binary_npz_path=BINARY_NPZ_PATH,\n",
        "    minhash_npz_path=MINHASH_NPZ_PATH,\n",
        "    n_bootstrap: int = 100,\n",
        "    rng_seed: int = 123,\n",
        "    theta_exact: float = 0.3,\n",
        "):\n",
        "    bin_data = np.load(binary_npz_path, allow_pickle=True)\n",
        "    X_bool = bin_data[\"X\"].astype(bool)\n",
        "    model_ids_all = bin_data[\"model_ids\"]\n",
        "\n",
        "    mh = np.load(minhash_npz_path, allow_pickle=True)\n",
        "    signatures = mh[\"signatures\"]          # (m, n)\n",
        "    m = signatures.shape[0]\n",
        "\n",
        "    # sweep all exact (b,r) with b*r = m\n",
        "    cfgs = all_band_configs(m)            # list of (t_imp, b, r), sorted by t_imp\n",
        "    K = len(cfgs)\n",
        "\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "    F1_mat = np.zeros((n_bootstrap, K), dtype=float)\n",
        "    frac_mat = np.zeros((n_bootstrap, K), dtype=float)\n",
        "\n",
        "    eff = 0\n",
        "    for _ in range(n_bootstrap):\n",
        "        n = X_bool.shape[0]\n",
        "        in_bag = rng.integers(0, n, size=n)\n",
        "        subset_idx = np.unique(in_bag)\n",
        "        if subset_idx.size < 2:\n",
        "            continue\n",
        "\n",
        "        # subset views (fixed for this bootstrap)\n",
        "        sig_sub = signatures[:, subset_idx]     # (m, n_sub)\n",
        "        X_sub   = X_bool[subset_idx]            # (n_sub, p)\n",
        "        mid_sub = model_ids_all[subset_idx]     # (n_sub,)\n",
        "        n_sub   = subset_idx.size\n",
        "\n",
        "        for k, (t_imp, b, r) in enumerate(cfgs):\n",
        "            # candidates = bucket collisions only\n",
        "            pairs = lsh_candidate_pairs(sig_sub, bands=b, rows_per_band=r)\n",
        "\n",
        "            if pairs.shape[0] == 0:\n",
        "                F1_mat[eff, k] = 0.0\n",
        "                frac_mat[eff, k] = 0.0\n",
        "                continue\n",
        "\n",
        "            frac_mat[eff, k] = pairs.shape[0] / (n_sub * (n_sub - 1) // 2)\n",
        "\n",
        "            i_loc = pairs[:, 0]\n",
        "            j_loc = pairs[:, 1]\n",
        "            sims_exact = exact_jaccard_on_candidates(X_sub, i_loc, j_loc)\n",
        "\n",
        "            labels_sub = cluster_with_threshold(\n",
        "                n_items=n_sub,\n",
        "                i_idx=i_loc,\n",
        "                j_idx=j_loc,\n",
        "                sims=sims_exact,\n",
        "                theta=theta_exact,\n",
        "            )\n",
        "\n",
        "            stats = pairwise_confusion_and_f1_subset(\n",
        "                labels=labels_sub,\n",
        "                model_ids=mid_sub,\n",
        "                subset_idx=np.arange(n_sub),\n",
        "            )\n",
        "            F1_mat[eff, k] = float(stats[\"F1\"])\n",
        "\n",
        "        eff += 1\n",
        "\n",
        "    if eff == 0:\n",
        "        raise RuntimeError(\"No effective bootstraps.\")\n",
        "\n",
        "    F1_mean = F1_mat[:eff].mean(axis=0)\n",
        "    frac_mean = frac_mat[:eff].mean(axis=0)\n",
        "\n",
        "    # sort by mean fraction\n",
        "    order = np.argsort(frac_mean)\n",
        "\n",
        "    return {\n",
        "        \"t_values\": np.array([cfgs[i][0] for i in order]),   # implied thresholds (for labels only)\n",
        "        \"F1_mean\": F1_mean[order],\n",
        "        \"frac_mean\": frac_mean[order],\n",
        "        \"n_effective_bootstrap\": eff,\n",
        "        \"cfgs_sorted\": [cfgs[i] for i in order],            # (t_imp,b,r) per point\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_finalF1_vs_fraction(curve: dict):\n",
        "    plt.figure()\n",
        "    plt.plot(\n",
        "        curve[\"frac_mean\"],\n",
        "        curve[\"F1_mean\"],\n",
        "        color=\"black\",\n",
        "        linewidth=2.5,\n",
        "        linestyle=\"-\",\n",
        "        marker=None,\n",
        "    )\n",
        "    plt.xlabel(\"Fraction of comparisons\")\n",
        "    plt.ylabel(\"F1-measure\")\n",
        "    plt.grid(False)\n",
        "    plt.title(None)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    curve = paper_style_curve_finalF1_vs_fraction(\n",
        "    binary_npz_path=BINARY_NPZ_PATH,\n",
        "    minhash_npz_path=MINHASH_NPZ_PATH,\n",
        "    n_bootstrap=5,\n",
        "    rng_seed=123,\n",
        "    theta_exact=0.3,\n",
        ")\n",
        "    print(\"effective bootstraps:\", curve[\"n_effective_bootstrap\"])\n",
        "    plot_finalF1_vs_fraction(curve)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tuXwU_GhEoy9",
        "outputId": "23a8cefe-dfa8-4e3b-f432-8fcee58df449"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1650199130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     curve = paper_style_curve_finalF1_vs_fraction(\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mbinary_npz_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBINARY_NPZ_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mminhash_npz_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMINHASH_NPZ_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1650199130.py\u001b[0m in \u001b[0;36mpaper_style_curve_finalF1_vs_fraction\u001b[0;34m(binary_npz_path, minhash_npz_path, n_bootstrap, rng_seed, theta_exact)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_imp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# candidates = bucket collisions only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsh_candidate_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows_per_band\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2345480513.py\u001b[0m in \u001b[0;36mlsh_candidate_pairs\u001b[0;34m(signatures, bands, rows_per_band)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m def minhash_jaccard(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}